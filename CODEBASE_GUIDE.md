# ğŸ“˜ Multi-Model Demand Forecasting - Codebase Guide

This document provides a comprehensive overview of the `airflow-demand-forecasting` project codebase. It explains the project structure, the purpose of each module, validation flows, and how to extend the system.

---

## ğŸ—ï¸ Project Architecture

The project follows a **modular, configuration-driven architecture** designed for scalability and maintainability.

### **Directory Structure**

```
airflow-demand-forecasting/
â”‚
â”œâ”€â”€ config/                  # âš™ï¸ Configuration (YAML)
â”‚   â”œâ”€â”€ base_config.yaml     # Global settings (paths, logging, validation)
â”‚   â”œâ”€â”€ prophet_config.yaml  # Prophet implementation details
â”‚   â”œâ”€â”€ sarima_config.yaml   # SARIMA specific settings
â”‚   â”œâ”€â”€ deepar_config.yaml   # DeepAR architecture & training params
â”‚   â””â”€â”€ ensemble_config.yaml # Ensemble strategy settings
â”‚
â”œâ”€â”€ scripts/                 # ğŸ Python Implementation
â”‚   â”œâ”€â”€ common/              # Shared utilities (Crucial!)
â”‚   â”‚   â”œâ”€â”€ config_loader.py # Loads & merges YAML configs
â”‚   â”‚   â”œâ”€â”€ metrics.py       # Performance calculations (MAPE, RMSE, etc.)
â”‚   â”‚   â”œâ”€â”€ model_versioning.py # Handles saving models with hash+seed
â”‚   â”‚   â”œâ”€â”€ data_validator.py # Checks data quality before training
â”‚   â”‚   â””â”€â”€ utils.py         # General helpers (logging, dates)
â”‚   â”‚
â”‚   â”œâ”€â”€ prophet/             # Facebook Prophet Module
â”‚   â”‚   â”œâ”€â”€ model_training.py
â”‚   â”‚   â””â”€â”€ prediction.py
â”‚   â”‚
â”‚   â”œâ”€â”€ sarima/              # SARIMA Module
â”‚   â”‚   â””â”€â”€ model_training.py # (Prediction logic handled here or implicitly)
â”‚   â”‚
â”‚   â”œâ”€â”€ deepar/              # AWS/GluonTS DeepAR Module
â”‚   â”‚   â”œâ”€â”€ model_training.py
â”‚   â”‚   â”œâ”€â”€ prediction.py
â”‚   â”‚   â”œâ”€â”€ external_features.py # Preprocessing for extra regressors
â”‚   â”‚   â””â”€â”€ data_formatting.py   # Converters for GluonTS format
â”‚   â”‚
â”‚   â””â”€â”€ ensemble/            # Ensemble Module
â”‚       â””â”€â”€ model_combiner.py # Logic to combine forecasts
â”‚
â”œâ”€â”€ dags/                    # ğŸŒªï¸ Airflow DAGs (Orchestration)
â”‚   â”œâ”€â”€ dag_prophet.py       # DAG for Prophet pipeline
â”‚   â”œâ”€â”€ dag_sarima.py        # DAG for SARIMA pipeline
â”‚   â”œâ”€â”€ dag_deepar.py        # DAG for DeepAR pipeline
â”‚   â””â”€â”€ dag_ensemble.py      # DAG that waits for others & combines results
â”‚
â”œâ”€â”€ tests/                   # ğŸ§ª Unit Tests
â”‚   â”œâ”€â”€ test_common/
â”‚   â”œâ”€â”€ test_prophet/
â”‚   â”œâ”€â”€ test_ensemble/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ models/                  # ğŸ’¾ Model Artifacts (Saved Models)
â”‚   â”œâ”€â”€ prophet/
â”‚   â”œâ”€â”€ sarima/ (organized by Product ID)
â”‚   â”œâ”€â”€ deepar/
â”‚   â””â”€â”€ ensemble/
â”‚
â”œâ”€â”€ data/                    # ğŸ“Š Data Store
â”‚   â”œâ”€â”€ raw/                 # Input CSVs
â”‚   â”œâ”€â”€ processed/           # Cleaned/Transformed data
â”‚   â””â”€â”€ predictions/         # Output forecasts
â”‚
â””â”€â”€ archive/                 # ğŸ“¦ Archived legacy files
```

---

## ğŸ”‘ Key Modules Explained

### **1. Configuration (`config/`)**
The system is controlled via YAML files. **Do not hardcode parameters in Python scripts.**
*   **`base_config.yaml`**: The source of truth for file paths and validation rules.
*   **Model Configs**: Each model has its own config file defining hyperparameters (e.g., `changepoint_prior_scale` for Prophet, `context_length` for DeepAR).

### **2. Common Utilities (`scripts/common/`)**
This is the backbone of the project.
*   **`config_loader.py`**: Reads YAMLs and enables hierarchical overrides.
*   **`model_versioning.py`**: Ensures reproducibility. Every saved model includes:
    *   **Timestamp**: When it was trained.
    *   **Data Hash**: Unique signature of the training data.
    *   **Seed**: Random seed used for initialization.
*   **`data_validator.py`**: Runs sanity checks (null values, schema validation) before any training job starts.

### **3. Model Implementations (`scripts/<model>/`)**
Each model is isolated in its own package.
*   **Prophet**: Standard implementation wrapping Facebook's library.
*   **SARIMA**: Handles **Per-Product** modeling (looping through unique IDs), as SARIMA is univariate.
*   **DeepAR**: Advanced implementation using PyTorch/GluonTS. Supports external text/numerical features via `external_features.py`.

### **4. Ensemble (`scripts/ensemble/`)**
This module reads predictions from individual models and combines them.
*   **`model_combiner.py`**: Implements the weighted average logic. It can optimized weights based on historical performance (Inverse Variance Weighting or OLS).

### **5. Orchestration (`dags/`)**
Airflow DAGs manage the workflow.
*   **Decoupled Execution**: Prophet, SARIMA, and DeepAR run in parallel DAGs.
*   **Dependencies**: The `dag_ensemble.py` uses `ExternalTaskSensor` to wait for the completion of the individual model DAGs before running.

---

## ğŸ› ï¸ How to Extend

### **Adding a New Model (e.g., XGBoost)**
1.  **Create Config**: Add `config/xgboost_config.yaml`.
2.  **Create Module**: Create `scripts/xgboost/` with `model_training.py` and `prediction.py`.
3.  **Implement Logic**: Use `common.model_versioning` to save artifacts.
4.  **Create DAG**: Add `dags/dag_xgboost.py`.
5.  **Update Ensemble**: Modify `scripts/ensemble/model_combiner.py` to include the new model's output in the weighting logic.
6.  **Add Dependencies**: Update `requirements/` (or create `requirements/xgboost.txt`).

---

## âœ… Best Practices

1.  **Always use `load_config()`**: Never manually modify paths in code.
2.  **Validate Data First**: Ensure your DAG calls `validate_data()` before training.
3.  **Version Everything**: Use the provided `create_model_version()` function.
4.  **Isolate Dependencies**: Keep `requirements/*.txt` clean to avoid conflicts between libraries (e.g., Statsmodels vs PyTorch).

---

## ğŸš€ Execution Flow

1.  **Installation**: Run `install_dependencies.sh` (WSL) or install from `requirements/`.
2.  **Data Prep**: Place raw data in `data/raw/` (or configure pipeline to fetch it).
3.  **Training**: Trigger `dag_prophet`, `dag_sarima`, `dag_deepar` in Airflow.
4.  **Ensembling**: Once training DAGs complete, `dag_ensemble` triggers automatically (or manually) to generate the final forecast.
